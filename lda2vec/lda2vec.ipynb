{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MulticoreTSNE\n",
      "  Downloading MulticoreTSNE-0.1.tar.gz (20 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\tf-gpu1.15\\lib\\site-packages (from MulticoreTSNE) (1.18.5)\n",
      "Requirement already satisfied: cffi in c:\\users\\user\\anaconda3\\envs\\tf-gpu1.15\\lib\\site-packages (from MulticoreTSNE) (1.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\envs\\tf-gpu1.15\\lib\\site-packages (from cffi->MulticoreTSNE) (2.20)\n",
      "Building wheels for collected packages: MulticoreTSNE\n",
      "  Building wheel for MulticoreTSNE (setup.py): started\n",
      "  Building wheel for MulticoreTSNE (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for MulticoreTSNE\n",
      "Failed to build MulticoreTSNE\n",
      "Installing collected packages: MulticoreTSNE\n",
      "    Running setup.py install for MulticoreTSNE: started\n",
      "    Running setup.py install for MulticoreTSNE: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\User\\anaconda3\\envs\\tf-gpu1.15\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-install-te8q1y8t\\\\MulticoreTSNE\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-install-te8q1y8t\\\\MulticoreTSNE\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-wheel-ea6nob1l'\n",
      "       cwd: C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-te8q1y8t\\MulticoreTSNE\\\n",
      "  Complete output (23 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.7\n",
      "  creating build\\lib.win-amd64-3.7\\MulticoreTSNE\n",
      "  copying MulticoreTSNE\\__init__.py -> build\\lib.win-amd64-3.7\\MulticoreTSNE\n",
      "  creating build\\lib.win-amd64-3.7\\MulticoreTSNE\\tests\n",
      "  copying MulticoreTSNE\\tests\\test_base.py -> build\\lib.win-amd64-3.7\\MulticoreTSNE\\tests\n",
      "  copying MulticoreTSNE\\tests\\__init__.py -> build\\lib.win-amd64-3.7\\MulticoreTSNE\\tests\n",
      "  running egg_info\n",
      "  writing MulticoreTSNE.egg-info\\PKG-INFO\n",
      "  writing dependency_links to MulticoreTSNE.egg-info\\dependency_links.txt\n",
      "  writing requirements to MulticoreTSNE.egg-info\\requires.txt\n",
      "  writing top-level names to MulticoreTSNE.egg-info\\top_level.txt\n",
      "  reading manifest file 'MulticoreTSNE.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  writing manifest file 'MulticoreTSNE.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  'cmake' is not recognized as an internal or external command,\n",
      "  operable program or batch file.\n",
      "  \n",
      "  Error: Cannot find cmake. Install cmake, e.g. `pip install cmake`.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for MulticoreTSNE\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\User\\anaconda3\\envs\\tf-gpu1.15\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-install-te8q1y8t\\\\MulticoreTSNE\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-install-te8q1y8t\\\\MulticoreTSNE\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-record-qzp0d8jq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\User\\anaconda3\\envs\\tf-gpu1.15\\Include\\MulticoreTSNE'\n",
      "         cwd: C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-te8q1y8t\\MulticoreTSNE\\\n",
      "    Complete output (23 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.7\n",
      "    creating build\\lib.win-amd64-3.7\\MulticoreTSNE\n",
      "    copying MulticoreTSNE\\__init__.py -> build\\lib.win-amd64-3.7\\MulticoreTSNE\n",
      "    creating build\\lib.win-amd64-3.7\\MulticoreTSNE\\tests\n",
      "    copying MulticoreTSNE\\tests\\test_base.py -> build\\lib.win-amd64-3.7\\MulticoreTSNE\\tests\n",
      "    copying MulticoreTSNE\\tests\\__init__.py -> build\\lib.win-amd64-3.7\\MulticoreTSNE\\tests\n",
      "    running egg_info\n",
      "    writing MulticoreTSNE.egg-info\\PKG-INFO\n",
      "    writing dependency_links to MulticoreTSNE.egg-info\\dependency_links.txt\n",
      "    writing requirements to MulticoreTSNE.egg-info\\requires.txt\n",
      "    writing top-level names to MulticoreTSNE.egg-info\\top_level.txt\n",
      "    reading manifest file 'MulticoreTSNE.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    writing manifest file 'MulticoreTSNE.egg-info\\SOURCES.txt'\n",
      "    running build_ext\n",
      "    'cmake' is not recognized as an internal or external command,\n",
      "    operable program or batch file.\n",
      "    \n",
      "    Error: Cannot find cmake. Install cmake, e.g. `pip install cmake`.\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\User\\anaconda3\\envs\\tf-gpu1.15\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-install-te8q1y8t\\\\MulticoreTSNE\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-install-te8q1y8t\\\\MulticoreTSNE\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-record-qzp0d8jq\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\User\\anaconda3\\envs\\tf-gpu1.15\\Include\\MulticoreTSNE' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install --user -U scikit-learn scipy matplotlib\n",
    "# !pip3 install nltk\n",
    "!pip3 install MulticoreTSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5645\n"
     ]
    }
   ],
   "source": [
    "with open('clean_total_text.txt', encoding='UTF-8') as fopen:\n",
    "    data = fopen.read().split('\\n')[:-1]\n",
    "\n",
    "ndata = []\n",
    "for d in data:\n",
    "    if(d):\n",
    "        ndata.append(d)\n",
    "\n",
    "data = ndata.copy()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "n_topics = 10\n",
    "embedding_size = 128\n",
    "epoch = 5\n",
    "switch_loss = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA2VEC:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_unique_documents,\n",
    "        vocab_size,\n",
    "        num_topics,\n",
    "        freqs,\n",
    "        embedding_size = 128,\n",
    "        num_sampled = 40,\n",
    "        learning_rate = 1e-3,\n",
    "        lmbda = 150.0,\n",
    "        alpha = None,\n",
    "        power = 0.75,\n",
    "        batch_size = 32,\n",
    "        clip_gradients = 5.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        moving_avgs = tf.train.ExponentialMovingAverage(0.9)\n",
    "        self.batch_size = batch_size\n",
    "        self.freqs = freqs\n",
    "        self.sess = tf.InteractiveSession()\n",
    "\n",
    "        self.X = tf.placeholder(tf.int32, shape = [None])\n",
    "        self.Y = tf.placeholder(tf.int64, shape = [None])\n",
    "        self.DOC = tf.placeholder(tf.int32, shape = [None])\n",
    "        step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "        self.switch_loss = tf.Variable(0, trainable = False)\n",
    "        train_labels = tf.reshape(self.Y, [-1, 1])\n",
    "        sampler = tf.nn.fixed_unigram_candidate_sampler(\n",
    "            train_labels,\n",
    "            num_true = 1,\n",
    "            num_sampled = num_sampled,\n",
    "            unique = True,\n",
    "            range_max = vocab_size,\n",
    "            distortion = power,\n",
    "            unigrams = self.freqs,\n",
    "        )\n",
    "\n",
    "        self.word_embedding = tf.Variable(\n",
    "            tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)\n",
    "        )\n",
    "        self.nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [vocab_size, embedding_size],\n",
    "                stddev = tf.sqrt(1 / embedding_size),\n",
    "            )\n",
    "        )\n",
    "        self.nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "        scalar = 1 / np.sqrt(num_unique_documents + num_topics)\n",
    "        self.doc_embedding = tf.Variable(\n",
    "            tf.random_normal(\n",
    "                [num_unique_documents, num_topics],\n",
    "                mean = 0,\n",
    "                stddev = 50 * scalar,\n",
    "            )\n",
    "        )\n",
    "        self.topic_embedding = tf.get_variable(\n",
    "            'topic_embedding',\n",
    "            shape = [num_topics, embedding_size],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.orthogonal_initializer(gain = scalar),\n",
    "        )\n",
    "        pivot = tf.nn.embedding_lookup(self.word_embedding, self.X)\n",
    "        proportions = tf.nn.embedding_lookup(self.doc_embedding, self.DOC)\n",
    "        doc = tf.matmul(proportions, self.topic_embedding)\n",
    "        doc_context = doc\n",
    "        word_context = pivot\n",
    "        context = tf.add(word_context, doc_context)\n",
    "        loss_word2vec = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights = self.nce_weights,\n",
    "                biases = self.nce_biases,\n",
    "                labels = self.Y,\n",
    "                inputs = context,\n",
    "                num_sampled = num_sampled,\n",
    "                num_classes = vocab_size,\n",
    "                num_true = 1,\n",
    "                sampled_values = sampler,\n",
    "            )\n",
    "        )\n",
    "        self.fraction = tf.Variable(1, trainable = False, dtype = tf.float32)\n",
    "\n",
    "        n_topics = self.doc_embedding.get_shape()[1].value\n",
    "        log_proportions = tf.nn.log_softmax(self.doc_embedding)\n",
    "        if alpha is None:\n",
    "            alpha = 1.0 / n_topics\n",
    "        loss = -(alpha - 1) * log_proportions\n",
    "        prior = tf.reduce_sum(loss)\n",
    "\n",
    "        loss_lda = lmbda * self.fraction * prior\n",
    "        self.cost = tf.cond(\n",
    "            step < self.switch_loss,\n",
    "            lambda: loss_word2vec,\n",
    "            lambda: loss_word2vec + loss_lda,\n",
    "        )\n",
    "        loss_avgs_op = moving_avgs.apply([loss_lda, loss_word2vec, self.cost])\n",
    "        with tf.control_dependencies([loss_avgs_op]):\n",
    "            self.optimizer = tf.contrib.layers.optimize_loss(\n",
    "                self.cost,\n",
    "                tf.train.get_global_step(),\n",
    "                learning_rate,\n",
    "                'Adam',\n",
    "                clip_gradients = clip_gradients,\n",
    "            )\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(\n",
    "        self, pivot_words, target_words, doc_ids, num_epochs, switch_loss = 3\n",
    "    ):\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        temp_fraction = self.batch_size / len(pivot_words)\n",
    "        self.sess.run(tf.assign(self.fraction, temp_fraction))\n",
    "        self.sess.run(tf.assign(self.switch_loss, switch_loss))\n",
    "        for e in range(num_epochs):\n",
    "            pbar = tqdm(\n",
    "                range(0, len(pivot_words), self.batch_size),\n",
    "                desc = 'minibatch loop',\n",
    "            )\n",
    "            for i in pbar:\n",
    "                batch_x = pivot_words[\n",
    "                    i : min(i + self.batch_size, len(pivot_words))\n",
    "                ]\n",
    "                batch_y = target_words[\n",
    "                    i : min(i + self.batch_size, len(pivot_words))\n",
    "                ]\n",
    "                batch_doc = doc_ids[\n",
    "                    i : min(i + self.batch_size, len(pivot_words))\n",
    "                ]\n",
    "                _, cost = self.sess.run(\n",
    "                    [self.optimizer, self.cost],\n",
    "                    feed_dict = {\n",
    "                        self.X: batch_x,\n",
    "                        self.Y: batch_y,\n",
    "                        self.DOC: batch_doc,\n",
    "                    },\n",
    "                )\n",
    "                pbar.set_postfix(cost = cost, epoch = e + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def skipgrams(\n",
    "    sequence,\n",
    "    vocabulary_size,\n",
    "    window_size = 4,\n",
    "    negative_samples = 1.0,\n",
    "    shuffle = True,\n",
    "    categorical = False,\n",
    "    sampling_table = None,\n",
    "    seed = None,\n",
    "):\n",
    "    couples = []\n",
    "    labels = []\n",
    "    for i, wi in enumerate(sequence):\n",
    "        if not wi:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        for j in range(window_start, window_end):\n",
    "            if j != i:\n",
    "                wj = sequence[j]\n",
    "                if not wj:\n",
    "                    continue\n",
    "                couples.append([wi, wj])\n",
    "                if categorical:\n",
    "                    labels.append([0, 1])\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    if negative_samples > 0:\n",
    "        num_negative_samples = int(len(labels) * negative_samples)\n",
    "        words = [c[0] for c in couples]\n",
    "        random.shuffle(words)\n",
    "\n",
    "        couples += [\n",
    "            [words[i % len(words)], random.randint(1, vocabulary_size - 1)]\n",
    "            for i in range(num_negative_samples)\n",
    "        ]\n",
    "        if categorical:\n",
    "            labels += [[1, 0]] * num_negative_samples\n",
    "        else:\n",
    "            labels += [0] * num_negative_samples\n",
    "\n",
    "    if shuffle:\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 10e6)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(couples)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(labels)\n",
    "\n",
    "    return couples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer().fit(data)\n",
    "transformed = bow.transform(data)\n",
    "idx_text_clean, len_idx_text_clean = [], []\n",
    "for text in transformed:\n",
    "    splitted = text.nonzero()[1]\n",
    "    idx_text_clean.append(splitted)\n",
    "    \n",
    "dictionary = {\n",
    "        i: no for no, i in enumerate(bow.get_feature_names())\n",
    "    }\n",
    "reversed_dictionary = {\n",
    "        no: i for no, i in enumerate(bow.get_feature_names())\n",
    "    }\n",
    "freqs = transformed.toarray().sum(axis = 0).tolist()\n",
    "doc_ids = np.arange(len(idx_text_clean))\n",
    "num_unique_documents = doc_ids.max()\n",
    "pivot_words, target_words, doc_ids = [], [], []\n",
    "for i, t in enumerate(idx_text_clean):\n",
    "    pairs, _ = skipgrams(\n",
    "            t,\n",
    "            vocabulary_size = len(dictionary),\n",
    "            window_size = window_size,\n",
    "            shuffle = True,\n",
    "            negative_samples = 0,\n",
    "        )\n",
    "    for pair in pairs:\n",
    "        temp_data = pair\n",
    "        pivot_words.append(temp_data[0])\n",
    "        target_words.append(temp_data[1])\n",
    "        doc_ids.append(i)\n",
    "pivot_words, target_words, doc_ids = shuffle(\n",
    "        pivot_words, target_words, doc_ids, random_state = 10\n",
    ")\n",
    "num_unique_documents = len(idx_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\steve\\.conda\\envs\\tf-gpu1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LDA2VEC(\n",
    "        num_unique_documents,\n",
    "        len(dictionary),\n",
    "        n_topics,\n",
    "        freqs,\n",
    "        embedding_size = embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|███████████████████████████████████████| 5669/5669 [01:09<00:00, 81.98it/s, cost=-1.1e+4, epoch=1]\n",
      "minibatch loop: 100%|██████████████████████████████████████| 5669/5669 [01:07<00:00, 84.08it/s, cost=-2.38e+4, epoch=2]\n",
      "minibatch loop: 100%|███████████████████████████████████████| 5669/5669 [01:07<00:00, 83.67it/s, cost=-3.7e+4, epoch=3]\n",
      "minibatch loop: 100%|██████████████████████████████████████| 5669/5669 [01:07<00:00, 83.89it/s, cost=-5.01e+4, epoch=4]\n",
      "minibatch loop: 100%|██████████████████████████████████████| 5669/5669 [01:07<00:00, 83.74it/s, cost=-6.33e+4, epoch=5]\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    pivot_words, target_words, doc_ids, epoch, switch_loss = switch_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embed = model.sess.run(model.doc_embedding)\n",
    "topic_embed = model.sess.run(model.topic_embedding)\n",
    "word_embed = model.sess.run(model.word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5645, 10), (10, 128), (9460, 128))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embed.shape, topic_embed.shape, word_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\.conda\\envs\\tf-gpu1.15\\lib\\site-packages\\sklearn\\utils\\validation.py:71: FutureWarning: Pass n_neighbors=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['suspended', 0.3317294120788574],\n",
       " ['tsla', 0.3313751816749573],\n",
       " ['somewhere', 0.32911866903305054],\n",
       " ['class', 0.32137012481689453],\n",
       " ['holiday', 0.3137776255607605],\n",
       " ['program', 0.31174319982528687],\n",
       " ['early_dec', 0.31152182817459106],\n",
       " ['coronavirusrelated', 0.31065189838409424],\n",
       " ['noted', 0.30999553203582764]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'beautiful'\n",
    "nn = NearestNeighbors(10, metric = 'cosine').fit(word_embed)\n",
    "distances, idx = nn.kneighbors(word_embed[dictionary[word]].reshape((1, -1)))\n",
    "word_list = []\n",
    "for i in range(1, idx.shape[1]):\n",
    "    word_list.append([reversed_dictionary[idx[0, i]], 1 - distances[0, i]])\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 1 : mainlander carefully implementation husband arslanhidayat festering excluding sampleread filters\n",
      "topic 2 : economical 的宿主是什么野生动物有多困难了 ducksfollow dow censored wards chinaunited risking recently\n",
      "topic 3 : 中国人 virologe forwarded fridaymorning jonlopezt tim graphs couldnt feast\n",
      "topic 4 : isolation downplaying bakhabarsavera cleaned requisitions charged chinafor antipyretics usfda\n",
      "topic 5 : esp dewsnewz galvanizes brace nieman hydax asianinsider tcoohfkmvocst certainty\n",
      "topic 6 : imdasg tcoorczjhffwv 引發大陸各地恐慌 trueislam partners admin hasnt tcohnmotxaote extensively\n",
      "topic 7 : senrickscott distraction longan small token distractions chinapurportedly entailshealth missiles\n",
      "topic 8 : tim gen counted todayread 武汉疫情_武汉肺炎追踪 tcoplabwnhsbh rape abandon sessional\n",
      "topic 9 : mileniolive yday unleash viruschine alibaba spotlight rps xianliang wuhanquaranted\n",
      "topic 10 : nontoxic strengthen advisory wuhanhe hospitalsanyone blogs zombielike stopgod zaobao\n"
     ]
    }
   ],
   "source": [
    "components = topic_embed.dot(word_embed.T)\n",
    "for no, topic in enumerate(components):\n",
    "    topic_string = ' '.join([reversed_dictionary[i]\n",
    "              for i in topic.argsort()[: -10 : -1]])\n",
    "    print('topic %d : %s'%(no + 1, topic_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MulticoreTSNE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-14363ed70144>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mMulticoreTSNE\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMulticoreTSNE\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'MulticoreTSNE'"
     ]
    }
   ],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_jobs=4)\n",
    "X = tsne.fit_transform(doc_embed.astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "unique_label = np.unique(trainset.target)\n",
    "encoded = LabelEncoder().fit_transform(trainset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "colors = ['red','blue']\n",
    "for no, i in enumerate(unique_label):\n",
    "    plt.scatter(X[encoded==i,0],X[encoded==i,1],label=unique_label[no],color=colors[no])\n",
    "plt.legend(['negative','positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
